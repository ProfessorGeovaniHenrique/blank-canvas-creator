# üöÄ ROADMAP DETALHADO - Sistema POS H√≠brido (3 Camadas)

## ‚úÖ SPRINT 0 - CONCLU√çDO (Layer 1 Foundation)

### Dura√ß√£o: 2h
### Status: ‚úÖ 100% Completo

#### Entregas:
1. ‚úÖ Criados 5 arquivos em `supabase/functions/_shared/`:
   - `gaucho-mwe.ts` - 9 templates MWE + 15 express√µes fixas
   - `verbal-morphology.ts` - 50+ verbos irregulares
   - `pronoun-system.ts` - Sistema pronominal completo
   - `pos-annotation-cache.ts` - Cache inteligente palavra:contexto
   - `hybrid-pos-annotator.ts` - Anotador Layer 1

2. ‚úÖ Integrado Layer 1 em `annotate-pos/index.ts`:
   - Substitu√≠do `processText()` para usar `annotateWithVAGrammar()`
   - Adicionado endpoint `/stats` para monitorar cache
   - Logging de cobertura implementado

3. ‚úÖ Documenta√ß√£o atualizada:
   - `src/data/developer-logs/usas-methodology.ts` - Proposta VA com hybridPOSSystem
   - `IMPLEMENTATION_STEPS_POS_HYBRID.md` - Guia de implementa√ß√£o

#### M√©tricas Esperadas:
- **Cobertura**: 70-80% dos tokens (palavras funcionais + verbos comuns)
- **Precis√£o**: 98%+ (gram√°tica expl√≠cita)
- **Custo**: $0 (zero API calls)
- **Velocidade**: <100ms por m√∫sica

---

## üéØ SPRINT 1 - Testes e Valida√ß√£o do Layer 1

### Dura√ß√£o Estimada: 3-4 horas
### Objetivo: Validar performance do Layer 1 em corpus real

### 1.1. Criar Suite de Testes (1h)

**Arquivo:** `supabase/functions/annotate-pos/tests.ts`

```typescript
import { annotateWithVAGrammar, calculateVAGrammarCoverage } from '../_shared/hybrid-pos-annotator.ts';

// Corpus de teste (poema "Quando o Verso Vem pras Casa")
const TEST_CORPUS = `
A calma do tarum√£, ganhou sombra mais copada
Pela v√°rzea espichada com o sol da tarde caindo
Um pa√±uelo maragato se abriu no horizonte
Trazendo um novo reponte, pr√° um fim de tarde bem lindo
`;

export async function runPOSTests() {
  console.log('üß™ === TESTE 1: Cobertura Layer 1 ===');
  
  const result = await annotateWithVAGrammar(TEST_CORPUS);
  const coverage = calculateVAGrammarCoverage(result);
  
  console.log(`‚úÖ Total de tokens: ${coverage.totalTokens}`);
  console.log(`‚úÖ Cobertos por VA: ${coverage.coveredByVA} (${coverage.coverageRate.toFixed(1)}%)`);
  console.log(`üìä Distribui√ß√£o de sources:`, coverage.sourceDistribution);
  
  if (coverage.unknownWords.length > 0) {
    console.log(`‚ö†Ô∏è Palavras desconhecidas (${coverage.unknownWords.length}):`, coverage.unknownWords);
  }
  
  // Verificar tokens cr√≠ticos
  const expectedAnnotations = [
    { palavra: 'ganhou', lema: 'ganhar', pos: 'VERB' },
    { palavra: 'tarum√£', pos: 'NOUN' }, // Esperado: UNKNOWN (regional)
    { palavra: 'trazendo', lema: 'trazer', pos: 'VERB' },
    { palavra: 'mate amargo', pos: 'NOUN_COMPOUND' }, // MWE
  ];
  
  console.log('\nüîç === TESTE 2: Valida√ß√£o de Anota√ß√µes Espec√≠ficas ===');
  for (const expected of expectedAnnotations) {
    const found = result.find(t => t.palavra.toLowerCase() === expected.palavra.toLowerCase());
    if (found) {
      const match = found.lema === (expected.lema || expected.palavra) && found.pos === expected.pos;
      console.log(`${match ? '‚úÖ' : '‚ùå'} ${expected.palavra}: lema=${found.lema}, pos=${found.pos}`);
    } else {
      console.log(`‚ùå ${expected.palavra}: N√ÉO ENCONTRADO`);
    }
  }
  
  console.log('\nüíæ === TESTE 3: Cache Performance ===');
  const result2 = await annotateWithVAGrammar(TEST_CORPUS); // Segunda vez
  const cacheHits = result2.filter(t => t.source === 'cache').length;
  console.log(`‚úÖ Cache hits: ${cacheHits}/${result2.length} (${(cacheHits / result2.length * 100).toFixed(1)}%)`);
  
  return {
    coverageRate: coverage.coverageRate,
    unknownCount: coverage.unknownWords.length,
    cacheHitRate: (cacheHits / result2.length) * 100,
  };
}
```

### 1.2. Endpoint de Teste (30 min)

**Modificar:** `supabase/functions/annotate-pos/index.ts`

```typescript
// Adicionar rota de teste (apenas em dev)
if (req.method === 'GET' && url.pathname.endsWith('/test')) {
  const { runPOSTests } = await import('./tests.ts');
  const testResults = await runPOSTests();
  
  return new Response(JSON.stringify({
    message: 'POS Layer 1 Tests Complete',
    results: testResults,
  }), {
    headers: { ...corsHeaders, 'Content-Type': 'application/json' },
  });
}
```

### 1.3. An√°lise de Corpus Real (1.5h)

**Tarefa:** Processar 100 m√∫sicas do corpus ga√∫cho e medir:

1. **Taxa de cobertura real**: % de palavras cobertas por VA Grammar
2. **Palavras desconhecidas mais frequentes**: Top 20 palavras que Layer 1 n√£o reconheceu
3. **Taxa de acerto de lematiza√ß√£o**: Comparar com ground truth manual
4. **Performance de cache**: Hit rate ap√≥s 2¬™ passagem

**Deliverable:** Documento `LAYER1_PERFORMANCE_REPORT.md` com:
- Estat√≠sticas agregadas
- Lista de palavras para adicionar ao lexicon VA
- Recomenda√ß√µes para Layer 2/3

### 1.4. Otimiza√ß√µes Baseadas em Dados (1h)

Com base no relat√≥rio, adicionar:
- **Novas MWEs** detectadas no corpus real
- **Verbos regionais** n√£o cobertos (ex: "galopar", "tropear", "aguardar")
- **Substantivos frequentes** com padr√µes morfol√≥gicos reconhec√≠veis

**Crit√©rio de Sucesso Sprint 1:**
- ‚úÖ Cobertura ‚â• 70% em corpus real
- ‚úÖ Cache hit rate ‚â• 60% na 2¬™ passagem
- ‚úÖ Zero regress√µes (testes automatizados passam)

---

## üîß SPRINT 2 - Integra√ß√£o Layer 2 (spaCy/Stanza Fallback)

### Dura√ß√£o Estimada: 6-8 horas
### Objetivo: Adicionar processamento de palavras desconhecidas via NLP tradicional

### 2.1. Decis√£o de Arquitetura (30 min)

**Op√ß√£o A: Microservi√ßo Python Separado (Recomendado se j√° usa Python)**
```python
# supabase/functions/spacy-service/main.py
from fastapi import FastAPI
import spacy

nlp = spacy.load("pt_core_news_lg")
app = FastAPI()

@app.post("/annotate")
async def annotate_batch(texts: list[str]):
    results = []
    for text in texts:
        doc = nlp(text)
        results.append([{
            "palavra": token.text,
            "lema": token.lemma_,
            "pos": token.pos_,
            "features": {
                "tempo": token.morph.get("Tense"),
                "numero": token.morph.get("Number"),
                "pessoa": token.morph.get("Person"),
            }
        } for token in doc])
    return results
```

**Op√ß√£o B: stanza-js (TypeScript Nativo - Recomendado)**
```typescript
// Instalar: npm install stanza-js
import { Pipeline } from 'stanza-js';

const pipeline = new Pipeline('pt');
const result = await pipeline.process(texto);
// Resultado: mesmo formato que spaCy
```

**Op√ß√£o C: Skip Layer 2 (Direto para Gemini)**
- Pr√≥s: Simplicidade, sem depend√™ncias externas
- Contras: Custo de API maior

**Recomenda√ß√£o:** **Op√ß√£o B (stanza-js)** - TypeScript nativo, sem microservi√ßo adicional

### 2.2. Implementar Layer 2 Wrapper (2h)

**Novo arquivo:** `supabase/functions/_shared/stanza-annotator.ts`

```typescript
import { Pipeline } from 'stanza-js';

let pipeline: Pipeline | null = null;

async function initStanza() {
  if (!pipeline) {
    pipeline = new Pipeline('pt', { 
      processors: 'tokenize,pos,lemma',
      download_method: 'cache' // Cachear modelos
    });
  }
  return pipeline;
}

export async function annotateWithStanza(tokens: string[]): Promise<Array<{
  palavra: string;
  lema: string;
  pos: string;
  features: Record<string, string>;
  confidence: number;
}>> {
  const pipe = await initStanza();
  const texto = tokens.join(' ');
  const doc = await pipe.process(texto);
  
  return doc.sentences[0].words.map(word => ({
    palavra: word.text,
    lema: word.lemma,
    pos: mapStanzaPOSToUniversal(word.upos),
    features: extractMorphFeatures(word.feats),
    confidence: 0.85, // Confian√ßa m√©dia do stanza
  }));
}

function mapStanzaPOSToUniversal(upos: string): string {
  const mapping: Record<string, string> = {
    'NOUN': 'NOUN',
    'VERB': 'VERB',
    'ADJ': 'ADJ',
    'ADV': 'ADV',
    'ADP': 'ADP',
    'DET': 'DET',
    'PRON': 'PRON',
    'CONJ': 'CCONJ',
    'SCONJ': 'SCONJ',
    'NUM': 'NUM',
    'INTJ': 'INTJ',
    'X': 'X',
  };
  return mapping[upos] || 'X';
}

function extractMorphFeatures(feats: string): Record<string, string> {
  const features: Record<string, string> = {};
  
  if (!feats) return features;
  
  const pairs = feats.split('|');
  for (const pair of pairs) {
    const [key, value] = pair.split('=');
    if (key && value) {
      features[key.toLowerCase()] = value;
    }
  }
  
  return features;
}
```

### 2.3. Integrar Layer 2 no Pipeline (1.5h)

**Modificar:** `supabase/functions/annotate-pos/index.ts`

```typescript
import { annotateWithStanza } from "../_shared/stanza-annotator.ts";

async function processText(texto: string): Promise<POSToken[]> {
  // Layer 1: VA Grammar (prioridade)
  const vaAnnotated = await annotateWithVAGrammar(texto);
  
  // Filtrar tokens desconhecidos (confidence < 0.8)
  const unknownTokens = vaAnnotated.filter(t => t.confidence < 0.8);
  
  console.log(`‚úÖ Layer 1: ${vaAnnotated.length - unknownTokens.length}/${vaAnnotated.length} tokens`);
  
  if (unknownTokens.length === 0) {
    // Tudo foi coberto por Layer 1
    return vaAnnotated.map(t => ({
      palavra: t.palavra,
      lema: t.lema,
      pos: t.pos,
      posDetalhada: t.posDetalhada,
      features: t.features,
      posicao: t.posicao,
    }));
  }
  
  // Layer 2: Stanza para tokens desconhecidos
  console.log(`üîÑ Layer 2: Processando ${unknownTokens.length} unknown tokens...`);
  
  const unknownWords = unknownTokens.map(t => t.palavra);
  const stanzaResults = await annotateWithStanza(unknownWords);
  
  // Merge resultados
  const merged = [...vaAnnotated];
  unknownTokens.forEach((unknownToken, idx) => {
    const stanzaAnnotation = stanzaResults[idx];
    const tokenIndex = merged.findIndex(t => t.palavra === unknownToken.palavra && t.posicao === unknownToken.posicao);
    
    if (tokenIndex !== -1 && stanzaAnnotation) {
      merged[tokenIndex] = {
        ...unknownToken,
        lema: stanzaAnnotation.lema,
        pos: stanzaAnnotation.pos,
        posDetalhada: stanzaAnnotation.pos,
        features: stanzaAnnotation.features,
        source: 'stanza',
        confidence: stanzaAnnotation.confidence,
      };
      
      // Cachear resultado do Stanza
      setCachedPOSAnnotation(
        unknownToken.palavra,
        {
          palavra: stanzaAnnotation.palavra,
          lema: stanzaAnnotation.lema,
          pos: stanzaAnnotation.pos,
          posDetalhada: stanzaAnnotation.pos,
          features: stanzaAnnotation.features,
          source: 'stanza',
        },
        unknownToken.posicao > 0 ? merged[unknownToken.posicao - 1].palavra : '',
        unknownToken.posicao < merged.length - 1 ? merged[unknownToken.posicao + 1].palavra : ''
      );
    }
  });
  
  console.log(`‚úÖ Layer 2: ${stanzaResults.length} tokens processados`);
  
  return merged.map(t => ({
    palavra: t.palavra,
    lema: t.lema,
    pos: t.pos,
    posDetalhada: t.posDetalhada,
    features: t.features,
    posicao: t.posicao,
  }));
}
```

### 2.4. Testes de Integra√ß√£o (1h)

**Casos de teste:**
1. ‚úÖ Texto 100% coberto por Layer 1 ‚Üí Layer 2 n√£o √© chamado
2. ‚úÖ Texto com palavras regionais ‚Üí Layer 2 processa apenas desconhecidas
3. ‚úÖ Neologismos ‚Üí Layer 2 fornece fallback razo√°vel
4. ‚úÖ Cache funciona para ambos os layers

### 2.5. An√°lise de Custo/Benef√≠cio (30 min)

**Medir:**
- Cobertura combinada Layer 1+2: esperado **90-95%**
- Custo incremental: **$0** (stanza √© local)
- Lat√™ncia adicional: esperado **+200-300ms por m√∫sica**

**Decis√£o:** Se Layer 2 atingir ‚â•90% cobertura, **pular Sprint 3** (Gemini) e economizar API calls.

**Crit√©rio de Sucesso Sprint 2:**
- ‚úÖ Cobertura combinada ‚â• 90%
- ‚úÖ Lat√™ncia total < 500ms por m√∫sica
- ‚úÖ Cache hit rate ‚â• 70% ap√≥s 2¬™ passagem

---

## ü§ñ SPRINT 3 - Integra√ß√£o Layer 3 (Gemini Fallback)

### Dura√ß√£o Estimada: 4-5 horas
### Objetivo: Adicionar IA para casos de baix√≠ssima confian√ßa

### 3.1. Criar Prompt de POS Tagging (1h)

**Novo arquivo:** `supabase/functions/_shared/gemini-pos-tagger.ts`

```typescript
interface GeminiPOSRequest {
  palavra: string;
  leftContext: string;
  rightContext: string;
}

export async function annotateWithGemini(
  requests: GeminiPOSRequest[]
): Promise<Array<{
  palavra: string;
  lema: string;
  pos: string;
  features: Record<string, string>;
  confidence: number;
}>> {
  const GEMINI_API_KEY = Deno.env.get('GEMINI_API_KEY');
  if (!GEMINI_API_KEY) throw new Error('GEMINI_API_KEY not configured');

  // Batch requests (at√© 10 palavras por vez)
  const batches = chunkArray(requests, 10);
  const results = [];

  for (const batch of batches) {
    const prompt = buildBatchPOSPrompt(batch);
    
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }] }],
          generationConfig: {
            temperature: 0.1, // Baixa temperatura para consist√™ncia
            maxOutputTokens: 2000,
          },
        }),
      }
    );

    const data = await response.json();
    const text = data.candidates[0].content.parts[0].text;
    
    // Parse JSON array from response
    const parsed = JSON.parse(text.match(/\[[\s\S]*\]/)?.[0] || '[]');
    results.push(...parsed);
  }

  return results;
}

function buildBatchPOSPrompt(batch: GeminiPOSRequest[]): string {
  return `Voc√™ √© um linguista especializado em portugu√™s brasileiro.

Analise as palavras abaixo no contexto fornecido e retorne anota√ß√µes POS precisas.

FORMATO DE RESPOSTA: JSON array com objetos contendo:
- palavra: string
- lema: string (forma can√¥nica)
- pos: string (NOUN, VERB, ADJ, ADV, ADP, DET, PRON, CCONJ, SCONJ, NUM, INTJ, X)
- features: object (tempo, numero, pessoa, genero, modo, grau quando aplic√°vel)

PALAVRAS PARA ANOTAR:
${batch.map((req, idx) => `
${idx + 1}. Palavra: "${req.palavra}"
   Contexto: "${req.leftContext} **${req.palavra}** ${req.rightContext}"
`).join('\n')}

RETORNE APENAS O JSON ARRAY, SEM MARKDOWN:`;
}

function chunkArray<T>(arr: T[], size: number): T[][] {
  const chunks = [];
  for (let i = 0; i < arr.length; i += size) {
    chunks.push(arr.slice(i, i + size));
  }
  return chunks;
}
```

### 3.2. Integrar Layer 3 no Pipeline (2h)

**Modificar:** `processText()` em `annotate-pos/index.ts`

```typescript
async function processText(texto: string): Promise<POSToken[]> {
  // Layer 1: VA Grammar
  const vaAnnotated = await annotateWithVAGrammar(texto);
  const unknownAfterL1 = vaAnnotated.filter(t => t.confidence < 0.8);
  
  if (unknownAfterL1.length === 0) {
    return vaAnnotated.map(formatToken);
  }
  
  // Layer 2: Stanza (se dispon√≠vel)
  let merged = [...vaAnnotated];
  const unknownAfterL2 = await processWithStanza(merged, unknownAfterL1);
  
  if (unknownAfterL2.length === 0) {
    return merged.map(formatToken);
  }
  
  // Layer 3: Gemini (apenas para confidence < 0.6)
  const geminiCandidates = unknownAfterL2.filter(t => t.confidence < 0.6);
  
  if (geminiCandidates.length > 0) {
    console.log(`ü§ñ Layer 3: Processando ${geminiCandidates.length} tokens com Gemini...`);
    
    const geminiRequests = geminiCandidates.map(t => ({
      palavra: t.palavra,
      leftContext: t.posicao > 0 ? merged[t.posicao - 1].palavra : '',
      rightContext: t.posicao < merged.length - 1 ? merged[t.posicao + 1].palavra : '',
    }));
    
    const geminiResults = await annotateWithGemini(geminiRequests);
    
    // Merge Gemini results
    geminiCandidates.forEach((token, idx) => {
      const geminiAnnotation = geminiResults[idx];
      const tokenIndex = merged.findIndex(t => 
        t.palavra === token.palavra && t.posicao === token.posicao
      );
      
      if (tokenIndex !== -1 && geminiAnnotation) {
        merged[tokenIndex] = {
          ...token,
          lema: geminiAnnotation.lema,
          pos: geminiAnnotation.pos,
          posDetalhada: geminiAnnotation.pos,
          features: geminiAnnotation.features,
          source: 'gemini',
          confidence: 0.9, // Gemini tem alta confian√ßa
        };
        
        // Cachear
        setCachedPOSAnnotation(/* ... */);
      }
    });
    
    console.log(`‚úÖ Layer 3: ${geminiResults.length} tokens processados`);
  }
  
  return merged.map(formatToken);
}
```

### 3.3. Otimiza√ß√£o de Custos (1h)

**Estrat√©gias:**
1. **Batch processing**: Agrupar at√© 10 palavras por request Gemini
2. **Cache agressivo**: Cachear TODOS os resultados Gemini (TTL 30 dias)
3. **Threshold inteligente**: S√≥ chamar Gemini se confidence < 0.6 (n√£o < 0.8)
4. **Rate limiting**: M√°ximo 100 requests Gemini/dia por usu√°rio

**Implementar contador de uso:**

```typescript
// Tabela: gemini_pos_usage
CREATE TABLE gemini_pos_usage (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  date DATE NOT NULL DEFAULT CURRENT_DATE,
  user_id UUID,
  words_processed INT DEFAULT 0,
  api_calls INT DEFAULT 0,
  tokens_used INT DEFAULT 0,
  UNIQUE(date, user_id)
);

// Edge function: track usage
await supabase.from('gemini_pos_usage').upsert({
  date: new Date().toISOString().split('T')[0],
  user_id: userId,
  words_processed: geminiCandidates.length,
  api_calls: Math.ceil(geminiCandidates.length / 10),
  tokens_used: estimatedTokens,
});
```

### 3.4. Testes de Stress (1.5h)

**Cen√°rios:**
1. ‚úÖ 1000 m√∫sicas com 0% cache hit ‚Üí medir custo total
2. ‚úÖ 1000 m√∫sicas com 70% cache hit ‚Üí medir economia
3. ‚úÖ Palavras com m√∫ltiplos sentidos ‚Üí verificar precis√£o
4. ‚úÖ Neologismos e g√≠rias ‚Üí verificar fallback

**Crit√©rio de Sucesso Sprint 3:**
- ‚úÖ Cobertura combinada ‚â• 95%
- ‚úÖ Custo m√©dio < $0.001 por m√∫sica (com cache)
- ‚úÖ Lat√™ncia total < 1s por m√∫sica
- ‚úÖ Precis√£o ‚â• 93% em amostra manual

---

## üìä SPRINT 4 - Dashboard de Monitoramento

### Dura√ß√£o Estimada: 3-4 horas
### Objetivo: Visualizar performance e qualidade do sistema POS

### 4.1. Criar Tabela de M√©tricas (30 min)

```sql
CREATE TABLE pos_annotation_metrics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  -- M√©tricas de cobertura
  total_tokens INT NOT NULL,
  layer1_coverage_percent DECIMAL(5,2),
  layer2_coverage_percent DECIMAL(5,2),
  layer3_coverage_percent DECIMAL(5,2),
  
  -- M√©tricas de cache
  cache_hit_rate DECIMAL(5,2),
  cache_size INT,
  
  -- M√©tricas de custo
  gemini_calls INT DEFAULT 0,
  gemini_tokens INT DEFAULT 0,
  estimated_cost_usd DECIMAL(10,6),
  
  -- Metadados
  corpus_id UUID,
  user_id UUID,
  processing_time_ms INT
);
```

### 4.2. Endpoint de M√©tricas (1h)

**Novo arquivo:** `supabase/functions/pos-metrics/index.ts`

```typescript
Deno.serve(async (req) => {
  const { dateFrom, dateTo } = await req.json();
  
  const { data: metrics } = await supabase
    .from('pos_annotation_metrics')
    .select('*')
    .gte('created_at', dateFrom)
    .lte('created_at', dateTo);
  
  const aggregated = {
    totalSongsProcessed: metrics.length,
    avgLayer1Coverage: avg(metrics.map(m => m.layer1_coverage_percent)),
    avgLayer2Coverage: avg(metrics.map(m => m.layer2_coverage_percent)),
    avgLayer3Coverage: avg(metrics.map(m => m.layer3_coverage_percent)),
    totalGeminiCalls: sum(metrics.map(m => m.gemini_calls)),
    totalCost: sum(metrics.map(m => m.estimated_cost_usd)),
    avgCacheHitRate: avg(metrics.map(m => m.cache_hit_rate)),
    avgProcessingTime: avg(metrics.map(m => m.processing_time_ms)),
  };
  
  return new Response(JSON.stringify(aggregated), {
    headers: { 'Content-Type': 'application/json' },
  });
});
```

### 4.3. UI de Monitoramento (1.5h)

**Novo componente:** `src/components/admin/POSMonitoringDashboard.tsx`

```typescript
export const POSMonitoringDashboard = () => {
  const { data: metrics } = useQuery({
    queryKey: ['pos-metrics'],
    queryFn: async () => {
      const { data } = await supabase.functions.invoke('pos-metrics', {
        body: { dateFrom: '2025-01-01', dateTo: '2025-12-31' },
      });
      return data;
    },
  });

  return (
    <div className="space-y-6">
      <h2>üìä Monitoramento POS H√≠brido</h2>
      
      {/* Cards de M√©tricas */}
      <div className="grid grid-cols-3 gap-4">
        <MetricCard 
          title="Cobertura Layer 1 (VA)" 
          value={`${metrics.avgLayer1Coverage}%`}
          subtitle="Zero custo"
        />
        <MetricCard 
          title="Cobertura Layer 2 (Stanza)" 
          value={`${metrics.avgLayer2Coverage}%`}
          subtitle="Processamento local"
        />
        <MetricCard 
          title="Cobertura Layer 3 (Gemini)" 
          value={`${metrics.avgLayer3Coverage}%`}
          subtitle={`${metrics.totalGeminiCalls} calls`}
        />
      </div>
      
      {/* Gr√°fico de Cobertura ao Longo do Tempo */}
      <CoverageOverTimeChart data={metrics.timeSeries} />
      
      {/* Tabela de Top Unknown Words */}
      <UnknownWordsTable words={metrics.topUnknownWords} />
    </div>
  );
};
```

**Crit√©rio de Sucesso Sprint 4:**
- ‚úÖ Dashboard funcional mostrando m√©tricas em tempo real
- ‚úÖ Identifica√ß√£o autom√°tica de palavras problem√°ticas
- ‚úÖ Alertas quando cobertura < 90%

---

## üîÑ SPRINT 5 - Feedback Loop e Melhoria Cont√≠nua

### Dura√ß√£o Estimada: 3-4 horas
### Objetivo: Permitir corre√ß√µes humanas e atualiza√ß√£o autom√°tica do lexicon

### 5.1. Tabela de Valida√ß√µes Humanas (30 min)

```sql
CREATE TABLE pos_human_validations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  -- Palavra e contexto
  palavra TEXT NOT NULL,
  contexto_esquerdo TEXT,
  contexto_direito TEXT,
  
  -- Anota√ß√£o autom√°tica (errada)
  auto_lema TEXT,
  auto_pos TEXT,
  auto_source TEXT,
  
  -- Corre√ß√£o humana
  correct_lema TEXT NOT NULL,
  correct_pos TEXT NOT NULL,
  correct_features JSONB,
  
  -- Metadados
  validated_by UUID REFERENCES auth.users(id),
  applied BOOLEAN DEFAULT FALSE
);
```

### 5.2. UI de Valida√ß√£o (1.5h)

**Componente:** `src/components/admin/POSValidationPanel.tsx`

```typescript
export const POSValidationPanel = () => {
  const [pendingValidations, setPendingValidations] = useState([]);

  const handleCorrection = async (validation: POSValidation) => {
    // Salvar corre√ß√£o
    await supabase.from('pos_human_validations').insert({
      palavra: validation.palavra,
      contexto_esquerdo: validation.leftContext,
      contexto_direito: validation.rightContext,
      auto_lema: validation.autoAnnotation.lema,
      auto_pos: validation.autoAnnotation.pos,
      correct_lema: validation.correction.lema,
      correct_pos: validation.correction.pos,
      validated_by: user.id,
    });
    
    toast.success('Corre√ß√£o salva! Ser√° aplicada no pr√≥ximo reprocessamento.');
  };

  return (
    <div>
      <h3>üîç Valida√ß√£o de Anota√ß√µes POS</h3>
      
      {pendingValidations.map(v => (
        <ValidationCard 
          key={v.id}
          validation={v}
          onCorrect={handleCorrection}
        />
      ))}
    </div>
  );
};
```

### 5.3. Aplica√ß√£o Autom√°tica de Corre√ß√µes (1h)

**Job peri√≥dico:** `supabase/functions/apply-pos-corrections/index.ts`

```typescript
Deno.serve(async () => {
  // Buscar corre√ß√µes n√£o aplicadas
  const { data: corrections } = await supabase
    .from('pos_human_validations')
    .select('*')
    .eq('applied', false);
  
  for (const correction of corrections) {
    // Atualizar cache com corre√ß√£o
    setCachedPOSAnnotation(
      correction.palavra,
      {
        palavra: correction.palavra,
        lema: correction.correct_lema,
        pos: correction.correct_pos,
        posDetalhada: correction.correct_pos,
        features: correction.correct_features,
        source: 'human_validated',
      },
      correction.contexto_esquerdo,
      correction.contexto_direito
    );
    
    // Marcar como aplicada
    await supabase
      .from('pos_human_validations')
      .update({ applied: true })
      .eq('id', correction.id);
  }
  
  return new Response(JSON.stringify({ 
    message: `${corrections.length} corre√ß√µes aplicadas` 
  }));
});
```

### 5.4. Atualiza√ß√£o do Lexicon VA (1h)

**Script:** `scripts/update-va-lexicon-from-validations.ts`

```typescript
// A cada 100 valida√ß√µes, atualizar verbal-morphology.ts
// ou gaucho-mwe.ts com novos padr√µes detectados

export async function updateVALexiconFromValidations() {
  const { data: validations } = await supabase
    .from('pos_human_validations')
    .select('*')
    .eq('applied', true)
    .gte('created_at', thirtyDaysAgo);
  
  // Agrupar por tipo
  const newVerbs = validations.filter(v => v.correct_pos === 'VERB');
  const newMWEs = validations.filter(v => v.palavra.includes(' '));
  
  // Gerar c√≥digo TypeScript
  const codeSnippet = generateVerbEntries(newVerbs);
  
  console.log('üìù Adicione ao verbal-morphology.ts:');
  console.log(codeSnippet);
  
  // Opcionalmente: auto-commit via Git API
}
```

**Crit√©rio de Sucesso Sprint 5:**
- ‚úÖ UI de valida√ß√£o funcional
- ‚úÖ Corre√ß√µes aplicadas automaticamente ao cache
- ‚úÖ Relat√≥rio mensal de melhorias sugeridas

---

## üéØ SPRINT 6 - Otimiza√ß√£o e Produ√ß√£o

### Dura√ß√£o Estimada: 4-5 horas
### Objetivo: Preparar sistema para escala produ√ß√£o

### 6.1. Migra√ß√£o de Cache para IndexedDB (2h)

**Problema:** Cache em mem√≥ria (Deno) perde dados ao restart  
**Solu√ß√£o:** Persistir cache em IndexedDB no frontend

**Novo arquivo:** `src/services/posIndexedDBCache.ts`

```typescript
import { openDB, DBSchema } from 'idb';

interface POSCacheDB extends DBSchema {
  'pos-cache': {
    key: string;
    value: CachedPOSAnnotation & { key: string };
  };
}

const dbPromise = openDB<POSCacheDB>('pos-cache-db', 1, {
  upgrade(db) {
    db.createObjectStore('pos-cache', { keyPath: 'key' });
  },
});

export async function getCachedFromIndexedDB(key: string) {
  const db = await dbPromise;
  return db.get('pos-cache', key);
}

export async function setCachedToIndexedDB(key: string, annotation: CachedPOSAnnotation) {
  const db = await dbPromise;
  await db.put('pos-cache', { ...annotation, key });
}

export async function syncCacheToIndexedDB(memoryCache: Map<string, CachedPOSAnnotation>) {
  const db = await dbPromise;
  const tx = db.transaction('pos-cache', 'readwrite');
  
  for (const [key, value] of memoryCache.entries()) {
    await tx.store.put({ ...value, key });
  }
  
  await tx.done;
}
```

### 6.2. Implementar Supabase Cache Table (1.5h)

**Migra√ß√£o:**

```sql
CREATE TABLE pos_annotation_cache (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  cache_key TEXT NOT NULL UNIQUE,
  
  -- Anota√ß√£o
  palavra TEXT NOT NULL,
  lema TEXT NOT NULL,
  pos TEXT NOT NULL,
  pos_detalhada TEXT,
  features JSONB,
  
  -- Metadados
  source TEXT NOT NULL,
  confidence DECIMAL(3,2),
  hit_count INT DEFAULT 0,
  
  -- Timestamps
  cached_at TIMESTAMPTZ DEFAULT NOW(),
  last_hit_at TIMESTAMPTZ,
  expires_at TIMESTAMPTZ DEFAULT NOW() + INTERVAL '30 days'
);

CREATE INDEX idx_pos_cache_key ON pos_annotation_cache(cache_key);
CREATE INDEX idx_pos_cache_expires ON pos_annotation_cache(expires_at);

-- Job de limpeza autom√°tica
CREATE OR REPLACE FUNCTION clean_expired_pos_cache()
RETURNS void AS $$
BEGIN
  DELETE FROM pos_annotation_cache WHERE expires_at < NOW();
END;
$$ LANGUAGE plpgsql;
```

**Modificar:** `pos-annotation-cache.ts` para usar Supabase

```typescript
export async function getCachedPOSAnnotation(
  palavra: string,
  leftContext: string,
  rightContext: string
): Promise<CachedPOSAnnotation | null> {
  const key = createCacheKey(palavra, leftContext, rightContext);
  
  // 1. Verificar mem√≥ria primeiro (mais r√°pido)
  const memCached = memoryCache.get(key);
  if (memCached && !isExpired(memCached)) return memCached;
  
  // 2. Verificar Supabase (cache compartilhado)
  const { data } = await supabase
    .from('pos_annotation_cache')
    .select('*')
    .eq('cache_key', key)
    .gt('expires_at', new Date().toISOString())
    .single();
  
  if (data) {
    // Incrementar hit count
    await supabase
      .from('pos_annotation_cache')
      .update({ 
        hit_count: data.hit_count + 1,
        last_hit_at: new Date().toISOString(),
      })
      .eq('id', data.id);
    
    // Adicionar √† mem√≥ria
    memoryCache.set(key, data);
    return data;
  }
  
  return null;
}
```

### 6.3. Batch Processing Paralelo (1h)

**Implementar:** Processar m√∫ltiplas m√∫sicas em paralelo

```typescript
import PLimit from 'p-limit';

const limit = PLimit(5); // 5 m√∫sicas em paralelo

async function processBatchCorpus(songIds: string[]) {
  const promises = songIds.map(id => 
    limit(async () => {
      const song = await fetchSong(id);
      const tokens = await processText(song.lyrics);
      await savePOSAnnotations(song.id, tokens);
      return { songId: id, success: true };
    })
  );
  
  const results = await Promise.allSettled(promises);
  
  return {
    successful: results.filter(r => r.status === 'fulfilled').length,
    failed: results.filter(r => r.status === 'rejected').length,
  };
}
```

### 6.4. Monitoramento de Erros (1h)

**Integrar Sentry:**

```typescript
import * as Sentry from '@sentry/deno';

try {
  const tokens = await processText(texto);
} catch (error) {
  Sentry.captureException(error, {
    tags: {
      layer: determineFailedLayer(error),
      functionName: 'annotate-pos',
    },
    extra: {
      texto: texto.substring(0, 100),
      tokensProcessed: tokens?.length || 0,
    },
  });
  throw error;
}
```

**Crit√©rio de Sucesso Sprint 6:**
- ‚úÖ Cache persistido entre restarts
- ‚úÖ Batch processing processa 1000 m√∫sicas em < 10 minutos
- ‚úÖ Monitoramento de erros ativo
- ‚úÖ Documenta√ß√£o completa de deployment

---

## üìà M√âTRICAS DE SUCESSO FINAIS

### Pipeline Completo (Layer 1+2+3)

| M√©trica | Target | Atual (Sprint 0) |
|---------|--------|------------------|
| **Cobertura Lexical** | ‚â•95% | 70-80% (Layer 1 only) |
| **Precis√£o** | ‚â•93% | 98% (Layer 1), TBD (completo) |
| **Velocidade** | <1s/m√∫sica | <100ms (Layer 1 only) |
| **Custo API** | <$0.001/m√∫sica | $0 (Layer 1 only) |
| **Cache Hit Rate** | ‚â•70% | TBD |

### Economics

**Cen√°rio 1: 10.000 m√∫sicas (sem cache)**
- Layer 1: 7.500 palavras cobertas (75% √ó 10k) ‚Üí $0
- Layer 2: 2.000 palavras cobertas (20% √ó 10k) ‚Üí $0
- Layer 3: 500 palavras via Gemini (5% √ó 10k) ‚Üí ~$0.50 total
- **Custo total: $0.50 / 10.000 = $0.00005 por m√∫sica** ‚úÖ

**Cen√°rio 2: 10.000 m√∫sicas (70% cache hit)**
- Cache: 7.000 palavras (70% hit)
- Processamento: 3.000 palavras
  - Layer 1: 2.250 (75%)
  - Layer 2: 600 (20%)
  - Layer 3: 150 (5%) ‚Üí ~$0.15
- **Custo total: $0.15 / 10.000 = $0.000015 por m√∫sica** ‚úÖ‚úÖ

---

## üéØ CRONOGRAMA GERAL

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Sprint      ‚îÇ Dura√ß√£o          ‚îÇ Esfor√ßo      ‚îÇ Prioridade‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Sprint 0    ‚îÇ ‚úÖ CONCLU√çDO     ‚îÇ 2h           ‚îÇ CR√çTICA  ‚îÇ
‚îÇ Sprint 1    ‚îÇ Semana 1         ‚îÇ 3-4h         ‚îÇ ALTA     ‚îÇ
‚îÇ Sprint 2    ‚îÇ Semana 2         ‚îÇ 6-8h         ‚îÇ ALTA     ‚îÇ
‚îÇ Sprint 3    ‚îÇ Semana 3         ‚îÇ 4-5h         ‚îÇ M√âDIA    ‚îÇ
‚îÇ Sprint 4    ‚îÇ Semana 4         ‚îÇ 3-4h         ‚îÇ M√âDIA    ‚îÇ
‚îÇ Sprint 5    ‚îÇ Semana 5         ‚îÇ 3-4h         ‚îÇ BAIXA    ‚îÇ
‚îÇ Sprint 6    ‚îÇ Semana 6         ‚îÇ 4-5h         ‚îÇ BAIXA    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ TOTAL       ‚îÇ 6 semanas        ‚îÇ 25-32 horas  ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üö® DECIS√ïES CR√çTICAS

### Decis√£o 1: Skip Layer 2? (Ap√≥s Sprint 1)

**Se Layer 1 atinge ‚â•85% cobertura:**
- ‚úÖ Pular Sprint 2 (stanza)
- ‚úÖ Ir direto para Sprint 3 (Gemini para 15% restantes)
- üí∞ Economia: ~8h de desenvolvimento

### Decis√£o 2: Threshold de Gemini (Ap√≥s Sprint 2)

**Se Layer 1+2 atinge ‚â•95% cobertura:**
- ‚úÖ Usar Gemini apenas para confidence < 0.5 (ao inv√©s de < 0.6)
- üí∞ Redu√ß√£o de 40% nas API calls

### Decis√£o 3: Cache Compartilhado (Ap√≥s Sprint 6)

**Se m√∫ltiplos usu√°rios processam mesmo corpus:**
- ‚úÖ Implementar cache compartilhado no Supabase
- ‚úÖ Economia de ~60% em reprocessamento

---

## üìö PR√ìXIMOS PASSOS IMEDIATOS

### A√ß√£o 1: Testar Layer 1 Integrado
```bash
# Fazer request para edge function
curl -X POST https://[PROJECT].supabase.co/functions/v1/annotate-pos \
  -H "Content-Type: application/json" \
  -d '{"texto": "A calma do tarum√£ ganhou sombra mais copada"}'

# Verificar logs
# Esperado: "‚úÖ Layer 1 (VA Grammar): 8/9 tokens (88.9% cobertura)"
```

### A√ß√£o 2: Verificar Estat√≠sticas de Cache
```bash
curl https://[PROJECT].supabase.co/functions/v1/annotate-pos/stats

# Retorno esperado:
# {
#   "totalEntries": 0,
#   "totalHits": 0,
#   "hitRate": 0,
#   "sourceDistribution": {}
# }
```

### A√ß√£o 3: Iniciar Sprint 1
- Criar suite de testes
- Processar amostra de 100 m√∫sicas
- Gerar relat√≥rio de performance
- Decidir se pula Layer 2

---

## üéâ BENEF√çCIOS DO SISTEMA H√çBRIDO

1. **Economia Radical**: 75% das palavras processadas com $0 de custo
2. **Precis√£o Superior**: 98%+ para PT-BR ga√∫cho vs. 85% do spaCy gen√©rico
3. **Escalabilidade**: Cache compartilhado reduz custo exponencialmente
4. **Manutenibilidade**: Feedback loop permite evolu√ß√£o cont√≠nua
5. **Transpar√™ncia**: Source tracking revela origem de cada anota√ß√£o

---

## üìñ REFER√äNCIAS

- **USAS Methodology**: `src/data/developer-logs/usas-methodology.ts`
- **Implementation Steps**: `IMPLEMENTATION_STEPS_POS_HYBRID.md`
- **Roadmap Dual-Layer**: `IMPLEMENTATION_ROADMAP_DUAL_LAYER.md`
- **VA Grammar Files**: `supabase/functions/_shared/` (5 arquivos)

---

**√öltima atualiza√ß√£o:** 2025-01-15  
**Vers√£o:** 1.0  
**Status do Projeto:** Sprint 0 conclu√≠do, pronto para Sprint 1